# WebCrawler

Web crawler, пошуковий робот чи веб-павук – програма, що є складовою частиною пошукової системи та призначена для обходу сторінок інтернету з метою занесення інформації до єдиної бази даних. 
Пошукові роботи здійснюють загальний пошук інформації в Інтернеті. Вони повідомляють про зміст знайденого документу, індексують його і добувають інформацію. Вони також переглядаю зображення, посилання та відправляють проіндексовану інформацію до бази даних пошукового механізму. 

Найчастіше роботів використовують для пошуку інформації та ресурсів. Там, де люди не можуть впоратись з величезною кількістю інформації, комп'ютер здійснює сортування та пошук швидко і якісно. Пошукові роботи переглядають дані на серверах і надають доступ до зібраної інформації за допомогою пошукової системи.
Користувач може одночасно використовувати перегляд та пошук для знаходження потрібної інформації. Навіть, якщо зібрана роботом база даних не містить саме ту інформацію, що потрібна, ймовірно в ній знайдуться сторінки з потрібними посиланнями.

Часто роботів використовують комбіновано - для виконання декількох задач. Наприклад, робот для web-вузла Вікіпедії - Інтернет-енциклопедії (uk.wikipedia.org ) в результаті пошуку видає перелік знайдених даних одночасно із зазначенням ступеня відповідності слову-запиту. 

Для робота одне з найважливіших значень має файл robots.txt, розташований на підконтрольному сервері. Зайшовши на будь-який сайт, робот звертається в першу чергу до нього. Цей файл – інструкція для робота. По-перше, robots.txt може взагалі не допустити бота на сайт і сайт залишиться не проіндексованим. По-друге, robots.txt може закрити боту доступ до певних сторінок і файлів.


АЛГОРИТМ РОБОТИ ПРОГРАМНОГО ЗАСТОСУНКУ
Для реалізації практичної частини проекту потрібно було створити наступні функції:
1.	Аналіз веб-сторінки (парсинг)
2.	Зібрання необхідної інформації зі сторінки
3.	Рекурсивний перехід за посиланнями 
4.	Зібрання інформації з інших сторінок
5.	Збереження інформації
Реалізація цих функцій відбувалась на мові програмування С++ та за допомогою зовнішніх бібліотек. Опис бібліотек наявний в наступних розділах 	
Розглянемо алгоритм роботи пошукового роботу на прикладі пошуку посилань на сайті:
1.	Перш за все повністю зчитується та зберігається веб-сторінка 
2.	Відбувається обхід сторінки, знаходження посилань
3.	Індексування посилань 
4.	Додавання посилань до потокобезпечної черги
5.	Паралельне взяття посилань с черги та їх обходження для знаходження посилань 
6.	Зберігання знайдених посилань 


ВИКОРИСТАНІ ЗОВНІШНІ ІНСТРУМЕНТИ 
Реалізацію програмного застосунку відбувалась на мові програмування С++. Скрипт для програми писався на мові програмування Пайтон. 

1.	CURL, LIBCURL 
cURL – крос-платформений програмний застосунок, що служить для передачі даних через Інтернет. Саме через цю бібліотеку можливо коректно створювати веб-запити та отримувати інформацію з веб-серверів. 
Для реалізації необхідних функції застосунку було використано бібліотеку libcurl, яка є дочірнім проектом cURL. Бібліотека Libcurl надає API для використання функцій cURL на таких мовах як С, Python, PHP тощо. 
Були використанні наступні функції бібліотеки LibCurl:
-	 curl_global_init() - ця функція встановлює програмне середовище, яке потребує libcurl.  Тобто це як завантажувач самої бібліотеки. Дана функція не є потокобезпечною!
-	 curl_easy_init() -  ця функція повинна бути першою функцією, яку потрібно викликати, і вона повертає переміну CURL , яку ви повинні використовувати як вхідні дані для інших функцій. Це повернення результат від запиту до серверу.  
-	 curl_easy_setopt() – ця функція використовується, щоб передати системі 
 «правила поведінки». Встановивши відповідні параметри, програма може змінити поведінку libcurl. 
-	 curl_easy_perform() – ця функція виконує запит блокуючим способом і повертає, або коли вдалося виконати запит, або коли повернуло помилку. 
-	 curl_easy_cleanup() - ця функція повинна бути останньою функцією, яка викликається для запиту. Це протилежна функції curl_easy_init і повинна бути викликана з тим же параметром, що і вхідний виклик.
2.	GUMBO PARSER
Gumbo – це HTML5 парсер від GOOGLE на мові програмування «С». Дана бібліотека дає можливість обходити «дерево» веб-сторінки для пошуку необхідної інформації. 
Дана бібліотека має свої переваги: 
-	Займає небагато місця 
-	API яке можливо обгорнути іншими мовами програмування
-	Підтримує фрагментний парсинг 
-	Повністю підтримує HTML5
Використані функції бібліотеки GUMBO:
-	GUMBONODE – створює HTML дерево, яке потім можливо обходити. 
-	 gumbo_get_attribute () – ця функція проходиться по веб-сторінці та повертає потрібні атрибути. 
Більш детальну інформацію про використанні бібліотеки можливо отримати на сайт офіційних документацій: 
-	cURL  - https://curl.haxx.se/libcurl/
-	GUMBO - http://matze.github.io/clib-doc/gumbo-parser/index.html




GITHUB WebCrawler: https://github.com/vromanko2/WebCrawler.git 

